"""OpenAI assistant tools wrappers and chat completions."""

import asyncio

import aiogram
import openai

from src.utils import bot_globals, output_formatting
from src.openai import openai_globals


class StreamEventHandler(openai.AsyncAssistantEventHandler):
    """Class for handling streamed events generated by assistants"""

    def __init__(self, message) -> None:
        super().__init__()
        self.message = message
        self.response = ""
        self.delta = 0
        self.chars_to_update = 0

    async def on_text_created(self, text) -> None:
        self.message = await self.message.answer("...")

    async def on_text_delta(self, delta, snapshot):
        self.response += delta.value
        self.delta += len(delta.value)
        if self.delta > self.chars_to_update:
            self.delta = 0
            self.chars_to_update = output_formatting.stream_increment(
                len(self.response)
            )
            self.message = await self.message.edit_text(self.response + "...")

    async def on_text_done(self, text):
        await self.message.edit_text(text.value)

    async def on_tool_call_created(self, tool_call):
        print("\nassistant > {tool_call.type}\n", flush=True)

    async def on_tool_call_delta(self, delta, snapshot):
        if delta.type == "code_interpreter":
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="", flush=True)
            if delta.code_interpreter.outputs:
                print("\n\noutput >", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        print(f"\n{output.logs}", flush=True)


async def stream_events(message: aiogram.types.Message, user: dict) -> None:
    """Wrapper over OpenAI's async streaming completion API call.

    Collect completion data and send it to user by editing the response multiple
    times after enough symbols is collected. If the resulted message is longer than
    4096 symbols (Telegram's restriction), split it into pieces and suggest to send
    the whole text at once as a txt file.
    """
    await bot_globals.bot.send_chat_action(
        message.chat.id, aiogram.enums.ChatAction.TYPING
    )
    await asyncio.sleep(0.15)
    async with openai_globals.client.beta.threads.runs.stream(
        thread_id=user["thread_id"],
        assistant_id=user["assistant_id"],
        event_handler=StreamEventHandler(message),
    ) as stream:
        await stream.until_done()
