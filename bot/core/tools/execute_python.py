"""Execute Python code tool using E2B Code Interpreter.

This module implements the execute_python tool for running AI-generated
Python code in secure sandboxed environments with internet access and
pip package installation support.

Phase 3.2+: Files generated by code execution are stored in Redis cache.
The model receives file metadata and decides whether to deliver files
to the user via the deliver_file tool.

NO __init__.py - use direct import:
    from core.tools.execute_python import execute_python, EXECUTE_PYTHON_TOOL
"""

import asyncio
import base64
import json
import re
from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING

from cache.exec_cache import store_exec_file
from core.clients import get_e2b_api_key
from core.mime_types import detect_mime_type
from core.pricing import calculate_e2b_cost
from core.pricing import cost_to_float
from e2b_code_interpreter import Sandbox
from utils.structured_logging import get_logger

if TYPE_CHECKING:
    from aiogram import Bot
    from sqlalchemy.ext.asyncio import AsyncSession

logger = get_logger(__name__)

# Valid pip package name pattern: name with optional version specifier
_PIP_PACKAGE_PATTERN = re.compile(r'^[a-zA-Z0-9]([a-zA-Z0-9._-]*[a-zA-Z0-9])?'
                                  r'(\[([a-zA-Z0-9._-]+(,[a-zA-Z0-9._-]+)*)\])?'
                                  r'([<>=!~]+[a-zA-Z0-9.*]+)?$')


def _validate_pip_packages(requirements_str: str) -> list[str]:
    """Validate pip package names to prevent command injection.

    Args:
        requirements_str: Space-separated package names.

    Returns:
        List of validated package names.

    Raises:
        ValueError: If any package name is invalid.
    """
    packages = requirements_str.split()
    invalid: list[str] = []
    for pkg in packages:
        if not _PIP_PACKAGE_PATTERN.match(pkg):
            invalid.append(pkg)
    if invalid:
        raise ValueError(f"Invalid pip package name(s): {', '.join(invalid)}. "
                         "Package names must match pattern: name[>=version]")
    return packages


def _sanitize_filename(filename: str) -> str:
    """Sanitize filename to prevent path traversal and null bytes.

    Args:
        filename: Raw filename from user input.

    Returns:
        Sanitized filename safe for filesystem use.
    """
    import os  # pylint: disable=import-outside-toplevel

    # Strip null bytes
    filename = filename.replace('\x00', '')
    # Use only basename (no path traversal)
    filename = os.path.basename(filename)
    # Remove any remaining unsafe characters
    filename = re.sub(r'[^a-zA-Z0-9._-]', '_', filename)
    # Ensure non-empty
    if not filename:
        filename = 'unnamed_file'
    return filename


def _run_sandbox_sync(  # pylint: disable=too-many-locals,too-many-statements
    code: str,
    downloaded_files: Dict[str, bytes],
    requirements: Optional[str],
    timeout: float,
    cached_sandbox_id: Optional[str] = None,
) -> Tuple[Dict[str, Any], float, Optional[str]]:
    """Run code in E2B sandbox synchronously.

    This function is designed to be called via asyncio.to_thread() to avoid
    blocking the event loop during sandbox operations.

    Supports sandbox reuse: if cached_sandbox_id is provided, attempts to
    reconnect to existing sandbox. On success, packages and files persist.

    Args:
        code: Python code to execute.
        downloaded_files: Pre-downloaded files as {filename: bytes}.
        requirements: Optional pip packages to install.
        timeout: Execution timeout in seconds.
        cached_sandbox_id: Optional sandbox ID to reconnect to.

    Returns:
        Tuple of (result_dict, sandbox_duration_seconds, sandbox_id).
        sandbox_id is returned for caching (sandbox is NOT killed on success).
    """
    import os  # pylint: disable=import-outside-toplevel
    import time  # pylint: disable=import-outside-toplevel

    api_key = get_e2b_api_key()
    os.environ["E2B_API_KEY"] = api_key

    sandbox_start_time = time.time()
    sandbox = None
    reused = False

    # Try to reconnect to existing sandbox if cached
    if cached_sandbox_id:
        try:
            sandbox = Sandbox.connect(cached_sandbox_id)
            reused = True
            logger.info("tools.execute_python.sandbox_reused",
                        sandbox_id=cached_sandbox_id)
        except Exception as reconnect_error:
            logger.info("tools.execute_python.sandbox_reconnect_failed",
                        sandbox_id=cached_sandbox_id,
                        error=str(reconnect_error))
            sandbox = None

    # Create new sandbox if no cached or reconnect failed
    if sandbox is None:
        sandbox = Sandbox.create()
        logger.info("tools.execute_python.sandbox_created",
                    sandbox_id=sandbox.sandbox_id)

    execution_failed = False
    try:
        # Upload pre-downloaded files to sandbox
        # Always upload files (even on reuse) as user may have new files
        if downloaded_files:
            sandbox.commands.run("mkdir -p /tmp/inputs")

            for filename, file_content in downloaded_files.items():
                filename = _sanitize_filename(filename)
                sandbox_path = f"/tmp/inputs/{filename}"
                sandbox.files.write(sandbox_path, file_content)
                logger.info("tools.execute_python.file_uploaded_to_sandbox",
                            filename=filename,
                            sandbox_path=sandbox_path,
                            size_bytes=len(file_content),
                            reused_sandbox=reused)

        # Install pip packages if specified
        # On reused sandbox, packages may already be installed but pip handles this
        if requirements:
            # Handle both string and list types (LLM may pass either)
            if isinstance(requirements, list):
                requirements_str = ' '.join(requirements)
            else:
                requirements_str = requirements

            # Validate package names to prevent command injection
            validated_packages = _validate_pip_packages(requirements_str)
            requirements_str = ' '.join(validated_packages)

            logger.info("tools.execute_python.installing_packages",
                        requirements=requirements,
                        reused_sandbox=reused)
            install_output = sandbox.commands.run(
                f"pip install {requirements_str}")
            logger.info("tools.execute_python.packages_installed",
                        exit_code=install_output.exit_code,
                        stdout_length=len(install_output.stdout))

        # Execute code
        logger.info("tools.execute_python.executing_code",
                    code_length=len(code),
                    timeout=timeout)

        execution = sandbox.run_code(code=code, timeout=timeout)

        # Process execution results
        stdout_list = execution.logs.stdout if execution.logs else []
        stderr_list = execution.logs.stderr if execution.logs else []

        logger.info("tools.execute_python.execution_complete",
                    stdout_lines=len(stdout_list),
                    stderr_lines=len(stderr_list),
                    has_error=bool(execution.error),
                    has_results=bool(execution.results))

        stdout_str = "".join(stdout_list)
        stderr_str = "".join(stderr_list)
        error_str = str(execution.error) if execution.error else ""
        success = execution.error is None

        # Serialize results
        results_list = []
        if execution.results:
            for r in execution.results:
                logger.debug("tools.execute_python.result_object",
                             result_type=type(r).__name__,
                             result_str=str(r)[:200])
                results_list.append(str(r)[:1000])

        results_serialized = json.dumps(results_list, ensure_ascii=False)

        # Scan for generated files
        generated_files: List[Dict[str, Any]] = []

        try:
            logger.info("tools.execute_python.scanning_output_files")
            all_files = sandbox.files.list("/tmp")

            for entry in all_files:
                if not entry.name:
                    continue

                file_path = entry.path

                if file_path.startswith("/tmp/inputs/"):
                    continue

                if entry.name in ("inputs", ".ICE-unix", ".X11-unix"):
                    continue

                # Skip hidden dirs and known system directories that
                # sandbox.files.read() can't read (raises "is a directory")
                if entry.name.startswith(".") or entry.name.startswith(
                        "systemd-private-"):
                    continue

                logger.info("tools.execute_python.downloading_output_file",
                            path=file_path,
                            name=entry.name)

                try:
                    file_bytes = sandbox.files.read(file_path, format="bytes")
                except Exception as read_error:  # pylint: disable=broad-exception-caught
                    logger.debug("tools.execute_python.skip_file",
                                 path=file_path,
                                 error=str(read_error))
                    continue

                # Detect MIME from magic bytes and extension (not just extension)
                mime_type = detect_mime_type(
                    filename=entry.name,
                    file_bytes=file_bytes,
                )

                generated_files.append({
                    "filename": entry.name,
                    "path": file_path,
                    "size": len(file_bytes),
                    "mime_type": mime_type,
                    "content": file_bytes
                })

                logger.info("tools.execute_python.output_file_found",
                            filename=entry.name,
                            size=len(file_bytes),
                            mime_type=mime_type)

            logger.info("tools.execute_python.output_files_scanned",
                        file_count=len(generated_files))

        except Exception as scan_error:  # pylint: disable=broad-exception-caught
            logger.info("tools.execute_python.output_scan_failed",
                        error=str(scan_error))

        sandbox_end_time = time.time()
        sandbox_duration = sandbox_end_time - sandbox_start_time

        # Prepare result dict
        # Phase 3.2+: Return raw file data for async caching
        # Files will be stored in Redis and metadata returned to Claude
        result: Dict[str, Any] = {
            "stdout": stdout_str,
            "stderr": stderr_str,
            "results": results_serialized,
            "error": error_str,
            "success": str(success).lower(),
        }

        # Include raw file data for async processing
        # (will be converted to cached files with metadata)
        if generated_files:
            result["_raw_files"] = generated_files

        # Return sandbox_id for caching (do NOT kill on success)
        return result, sandbox_duration, sandbox.sandbox_id

    except Exception as exec_error:
        # Mark execution as failed so we kill sandbox in finally
        execution_failed = True
        raise exec_error

    finally:
        # Only kill sandbox on execution failure to avoid stale state
        # On success, sandbox is kept alive for reuse (E2B auto-expires after idle)
        if execution_failed:
            try:
                sandbox.kill()
                logger.info("tools.execute_python.sandbox_killed_on_error")
            except Exception as cleanup_error:  # pylint: disable=broad-exception-caught
                logger.info("tools.execute_python.sandbox_cleanup_failed",
                            error=str(cleanup_error))
        else:
            logger.debug("tools.execute_python.sandbox_kept_alive",
                         sandbox_id=sandbox.sandbox_id)


# pylint: disable=too-many-locals,too-many-statements
async def execute_python(code: str,
                         bot: 'Bot',
                         session: 'AsyncSession',
                         thread_id: Optional[int] = None,
                         file_inputs: Optional[List[Dict[str, str]]] = None,
                         requirements: Optional[str] = None,
                         timeout: Optional[float] = 3600.0) -> Dict[str, Any]:
    """Execute Python code in E2B sandbox with file support.

    Runs Python code in secure E2B sandbox (Linux, Python 3.11+, headless).
    Supports pip packages, internet access, input/output file operations.
    Sandboxes are ephemeral and destroyed after execution.

    Environment:
    - Working directory: /home/user
    - Input files (if file_inputs provided): /tmp/inputs/{filename}
    - Output files: Save to /tmp/ or subdirectories
    - Bot auto-detects and downloads all new files from /tmp/

    Args:
        code: Python code to execute. Can include imports, multiple
            lines, functions, classes, etc.
        bot: Telegram Bot instance for downloading user files.
        session: Database session for querying file metadata.
        file_inputs: Optional list of input files to upload to sandbox.
            Each item: {"file_id": "file_abc...", "name": "document.pdf"}.
            Files will be available at /tmp/inputs/{name}.
        requirements: Optional space-separated list of pip packages
            (e.g., "numpy pandas matplotlib requests").
        timeout: Maximum execution time in seconds. Default: 3600 seconds
            (1 hour). Can be reduced for faster feedback on simple tasks.

    Returns:
        Dictionary with execution results:
        - 'stdout': Standard output from the code.
        - 'stderr': Standard error output (warnings, errors).
        - 'results': Serialized results (matplotlib plots, return values).
        - 'error': Error message if execution failed, otherwise empty.
        - 'success': 'true' if no errors, 'false' otherwise.
        - 'output_files': List of cached files with metadata:
            [{"temp_id": "exec_abc123_output.pdf", "filename": "output.pdf",
              "size_bytes": 102400, "mime_type": "application/pdf",
              "preview": "PDF document, ~5 pages, 100.0 KB"}]
        - 'cost_usd': Execution cost in USD.

    Raises:
        Exception: If sandbox creation, file operations, or execution fails.

    Examples:
        >>> # Simple execution
        >>> result = await execute_python(code="print('Hello!')")
        >>> print(result['stdout'])
        Hello!

        >>> # With input files
        >>> result = await execute_python(
        ...     code=("import pandas as pd; "
        ...           "df = pd.read_csv('/tmp/inputs/data.csv'); "
        ...           "print(df.head())"),
        ...     file_inputs=[{"file_id": "file_xyz", "name": "data.csv"}],
        ...     requirements="pandas"
        ... )

        >>> # With output files
        >>> result = await execute_python(
        ...     code="with open('/tmp/output.txt', 'w') as f: f.write('Result')"
        ... )
        >>> print(result['generated_files'])
        [{"filename": "output.txt", "size": 6, "mime_type": "text/plain"}]
    """
    try:
        logger.info("tools.execute_python.called",
                    code_length=len(code),
                    file_inputs_count=len(file_inputs or []),
                    requirements=requirements,
                    timeout=timeout)

        # Early validation: detect missing file_inputs when code expects files
        # This provides a clear error message instead of FileNotFoundError
        if '/tmp/inputs/' in code and not file_inputs:
            logger.info("tools.execute_python.missing_file_inputs",
                        code_preview=code[:200])
            return {
                "stdout": "",
                "stderr": "",
                "results": "[]",
                "error": (
                    "Code references '/tmp/inputs/' but no file_inputs provided. "
                    "You must pass file_inputs parameter with file_id and name "
                    "from 'Available files' section to upload files to sandbox. "
                    "Example: file_inputs=[{\"file_id\": \"file_abc...\", "
                    "\"name\": \"data.csv\"}]"),
                "success": "false",
                "generated_files": "[]",
                "cost_usd": "0.000000",
            }

        # Step 1: Check for cached sandbox (allows reuse between calls)
        # pylint: disable=import-outside-toplevel
        from cache.sandbox_cache import cache_sandbox
        from cache.sandbox_cache import get_cached_sandbox
        from cache.sandbox_cache import invalidate_sandbox
        cached_sandbox_id = None
        if thread_id:
            cached_sandbox_id = await get_cached_sandbox(thread_id)

        # Step 2: Download all input files IN PARALLEL before sandbox
        # This allows event loop to handle keepalive during downloads
        # Phase 3.2+: Using unified FileManager for all downloads
        downloaded_files: Dict[str, bytes] = {}
        if file_inputs:
            logger.info("tools.execute_python.downloading_input_files",
                        file_count=len(file_inputs))

            # Use FileManager for parallel downloads with caching
            from core.file_manager import FileManager
            file_manager = FileManager(bot, session)
            downloaded_files = await file_manager.download_many_by_claude_id(
                file_inputs,
                use_cache=True,
            )

        # Step 3: Run sandbox in thread pool to avoid blocking event loop
        # This allows keepalive updates during long sandbox operations
        # Pass cached_sandbox_id for reuse (faster iterations)
        try:
            result, sandbox_duration, sandbox_id = await asyncio.to_thread(
                _run_sandbox_sync,
                code,
                downloaded_files,
                requirements,
                timeout or 3600.0,
                cached_sandbox_id,
            )

            # Cache sandbox for reuse on success
            if thread_id and sandbox_id:
                await cache_sandbox(thread_id, sandbox_id)

        except Exception as sandbox_error:
            # Invalidate cached sandbox on error (stale state)
            if thread_id:
                await invalidate_sandbox(thread_id)
            raise sandbox_error

        # Step 4: Add cost to result
        cost_usd = calculate_e2b_cost(sandbox_duration)
        result["cost_usd"] = f"{cost_to_float(cost_usd):.6f}"

        # Step 5: Process generated files - store in Redis cache IN PARALLEL
        # Phase 3.2+: Model decides whether to deliver files to user
        # Phase 3.2+: Parallelized caching for better latency
        raw_files = result.pop("_raw_files", [])
        output_files: List[Dict[str, Any]] = []

        if raw_files:
            import uuid as uuid_mod  # pylint: disable=import-outside-toplevel
            execution_id = uuid_mod.uuid4().hex[:8]

            # Create cache tasks for parallel execution
            async def cache_one(
                    file_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
                """Cache single file and return metadata.

                For image files, also includes base64 data for Claude to see.
                """
                filename = file_data["filename"]
                content = file_data["content"]
                mime_type = file_data["mime_type"]

                # Context describes what this file is (for model understanding)
                file_context = f"Python output: {filename}"

                metadata = await store_exec_file(
                    filename=filename,
                    content=content,
                    mime_type=mime_type,
                    context=file_context,
                    execution_id=execution_id,
                    thread_id=thread_id,
                )

                if metadata:
                    logger.info(
                        "tools.execute_python.file_cached",
                        temp_id=metadata["temp_id"],
                        filename=filename,
                        size_bytes=len(content),
                    )
                    file_info: Dict[str, Any] = {
                        "temp_id": metadata["temp_id"],
                        "filename": metadata["filename"],
                        "size_bytes": metadata["size_bytes"],
                        "mime_type": metadata["mime_type"],
                        "preview": metadata["preview"],
                    }
                    # Add base64 image data for Claude to see image previews
                    # Limited to images under 1MB to avoid context bloat
                    if mime_type.startswith("image/") and len(
                            content) < 1024 * 1024:
                        file_info["_image_data"] = base64.b64encode(
                            content).decode('utf-8')
                    return file_info
                else:
                    logger.info(
                        "tools.execute_python.file_cache_failed",
                        filename=filename,
                        size_bytes=len(content),
                    )
                    return None

            # Cache all files in parallel
            cache_results = await asyncio.gather(
                *[cache_one(fd) for fd in raw_files],
                return_exceptions=True,
            )

            # Collect successful results
            for result_item in cache_results:
                if isinstance(result_item, BaseException):
                    logger.info(
                        "tools.execute_python.file_cache_exception",
                        error=str(result_item),
                    )
                elif result_item is not None:
                    output_files.append(result_item)  # type: ignore[arg-type]

        # Add output_files to result (metadata only, no binary)
        # Extract image previews for Claude to see
        image_previews = []
        for file_info in output_files:
            image_data = file_info.pop("_image_data", None)
            if image_data:
                image_previews.append({
                    "data": image_data,
                    "media_type": file_info["mime_type"],
                    "filename": file_info["filename"],
                })

        result["output_files"] = output_files

        # Add image previews for Claude to visually verify generated images
        if image_previews:
            result["_image_previews"] = image_previews
            logger.info(
                "tools.execute_python.image_previews_included",
                count=len(image_previews),
                filenames=[p["filename"] for p in image_previews],
            )

        logger.info("tools.execute_python.success",
                    success=result["success"],
                    stdout_length=len(result["stdout"]),
                    stderr_length=len(result["stderr"]),
                    output_files_count=len(output_files),
                    has_error=bool(result["error"]),
                    sandbox_duration_seconds=round(sandbox_duration, 2),
                    cost_usd=cost_to_float(cost_usd))

        return result

    except Exception as e:
        # E2B sandbox failures are external service issues, not our bugs
        logger.info("tools.execute_python.failed", error=str(e))
        # Re-raise to let caller handle
        raise


# Tool definition for Claude API (anthropic tools format)
# Tools are implicitly cached as part of the prompt prefix (before system prompt breakpoint)
EXECUTE_PYTHON_TOOL = {
    "name":
        "execute_python",
    "description":
        """Execute Python in E2B sandbox with file I/O and internet.

<environment>
Python 3.11+, Debian Linux, pip + apt-get available.
- Input files: /tmp/inputs/{filename} (from file_inputs parameter)
- Output files: Save to /tmp/, returned as output_files with temp_id
- System packages: subprocess.run('apt-get update && apt-get install -y ffmpeg libreoffice imagemagick', shell=True)
</environment>

<file_workflow>
1. file_inputs=[{"file_id": "file_xxx", "name": "doc.pdf"}] ‚Üí files at /tmp/inputs/
2. Code saves to /tmp/output.png ‚Üí output_files: [{temp_id: "exec_xxx", preview: "..."}]
3. Verify with preview_file(file_id="exec_xxx") if needed (does NOT send to user)
4. Send with deliver_file(temp_id="exec_xxx")
</file_workflow>

<output_files>
Returns metadata: temp_id, filename, size_bytes, mime_type, preview.
Images <1MB include base64 preview for visual verification.
Files cached 30 min.
</output_files>

Cost: ~$0.05/hour. Typical <1s = <$0.0001.""",
    "input_schema": {
        "type": "object",
        "properties": {
            "code": {
                "type":
                    "string",
                "description": (
                    "Python code to execute. Can include imports, multiple "
                    "lines, functions, classes, etc. Input files at /tmp/inputs/, "
                    "save outputs to /tmp/. Use print() for debug output.")
            },
            "file_inputs": {
                "type":
                    "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "file_id": {
                            "type":
                                "string",
                            "description": (
                                "claude_file_id from 'Available files' section "
                                "in system prompt (e.g., 'file_abc123...').")
                        },
                        "name": {
                            "type":
                                "string",
                            "description": (
                                "Original filename from 'Available files' "
                                "(e.g., 'document.pdf'). File will be available "
                                "at /tmp/inputs/{name} in sandbox.")
                        }
                    },
                    "required": ["file_id", "name"]
                },
                "description":
                    ("List of input files to upload to sandbox. "
                     "Use file_id and name from 'Available files' section. "
                     "Files uploaded to /tmp/inputs/ before code execution. "
                     "Optional - omit if no file inputs needed.")
            },
            "requirements": {
                "type":
                    "string",
                "description": (
                    "Space-separated list of pip packages to install before "
                    "execution (e.g., 'numpy pandas matplotlib requests pillow'). "
                    "Only packages not in Python standard library. Optional.")
            },
            "timeout": {
                "type":
                    "number",
                "description": (
                    "Maximum execution time in seconds. Default: 3600 (1 hour). "
                    "Can be reduced for faster feedback on simple tasks. Optional."
                )
            }
        },
        "required": ["code"]
    },
    # No cache_control needed ‚Äî tools are implicitly part of the cached prefix
    # (Anthropic caches: tools ‚Üí system ‚Üí messages, breakpoint is on system)
}


def format_execute_python_result(
    tool_input: Dict[str, Any],
    result: Dict[str, Any],
) -> str:
    """Format execute_python result for user display.

    Args:
        tool_input: The input parameters (code, file_inputs, etc.).
        result: The result dictionary with stdout, stderr, success, error.

    Returns:
        Formatted system message string.
    """
    if result.get("success") == "true":
        stdout = result.get("stdout", "").strip()
        if stdout:
            # Truncate long output
            preview = stdout[:100] + "..." if len(stdout) > 100 else stdout
            return f"[‚úÖ –ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω: {preview}]"
        return "[‚úÖ –ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ]"
    else:
        error = result.get("error", "unknown error")
        # Truncate long error
        preview = error[:80] + "..." if len(error) > 80 else error
        return f"[‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {preview}]"


# Unified tool configuration
# Import here to avoid circular dependencies at module level
from core.tools.base import ToolConfig  # pylint: disable=wrong-import-position

TOOL_CONFIG = ToolConfig(
    name="execute_python",
    definition=EXECUTE_PYTHON_TOOL,
    executor=execute_python,
    emoji="üêç",
    needs_bot_session=True,
    format_result=format_execute_python_result,
)
