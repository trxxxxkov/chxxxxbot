"""Execute Python code tool using E2B Code Interpreter.

This module implements the execute_python tool for running AI-generated
Python code in secure sandboxed environments with internet access and
pip package installation support.

Phase 3.2+: Files generated by code execution are stored in Redis cache.
The model receives file metadata and decides whether to deliver files
to the user via the deliver_file tool.

NO __init__.py - use direct import:
    from core.tools.execute_python import execute_python, EXECUTE_PYTHON_TOOL
"""

import asyncio
import base64
import json
from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING

from cache.exec_cache import store_exec_file
from core.clients import get_e2b_api_key
from core.mime_types import detect_mime_type
from core.pricing import calculate_e2b_cost
from core.pricing import cost_to_float
from e2b_code_interpreter import Sandbox
from utils.structured_logging import get_logger

if TYPE_CHECKING:
    from aiogram import Bot
    from sqlalchemy.ext.asyncio import AsyncSession

logger = get_logger(__name__)


def _run_sandbox_sync(  # pylint: disable=too-many-locals,too-many-statements
    code: str,
    downloaded_files: Dict[str, bytes],
    requirements: Optional[str],
    timeout: float,
    cached_sandbox_id: Optional[str] = None,
) -> Tuple[Dict[str, Any], float, Optional[str]]:
    """Run code in E2B sandbox synchronously.

    This function is designed to be called via asyncio.to_thread() to avoid
    blocking the event loop during sandbox operations.

    Supports sandbox reuse: if cached_sandbox_id is provided, attempts to
    reconnect to existing sandbox. On success, packages and files persist.

    Args:
        code: Python code to execute.
        downloaded_files: Pre-downloaded files as {filename: bytes}.
        requirements: Optional pip packages to install.
        timeout: Execution timeout in seconds.
        cached_sandbox_id: Optional sandbox ID to reconnect to.

    Returns:
        Tuple of (result_dict, sandbox_duration_seconds, sandbox_id).
        sandbox_id is returned for caching (sandbox is NOT killed on success).
    """
    import os  # pylint: disable=import-outside-toplevel
    import time  # pylint: disable=import-outside-toplevel

    api_key = get_e2b_api_key()
    os.environ["E2B_API_KEY"] = api_key

    sandbox_start_time = time.time()
    sandbox = None
    reused = False

    # Try to reconnect to existing sandbox if cached
    if cached_sandbox_id:
        try:
            sandbox = Sandbox.connect(cached_sandbox_id)
            reused = True
            logger.info("tools.execute_python.sandbox_reused",
                        sandbox_id=cached_sandbox_id)
        except Exception as reconnect_error:
            logger.info("tools.execute_python.sandbox_reconnect_failed",
                        sandbox_id=cached_sandbox_id,
                        error=str(reconnect_error))
            sandbox = None

    # Create new sandbox if no cached or reconnect failed
    if sandbox is None:
        sandbox = Sandbox.create()
        logger.info("tools.execute_python.sandbox_created",
                    sandbox_id=sandbox.sandbox_id)

    execution_failed = False
    try:
        # Upload pre-downloaded files to sandbox
        # Always upload files (even on reuse) as user may have new files
        if downloaded_files:
            sandbox.commands.run("mkdir -p /tmp/inputs")

            for filename, file_content in downloaded_files.items():
                sandbox_path = f"/tmp/inputs/{filename}"
                sandbox.files.write(sandbox_path, file_content)
                logger.info("tools.execute_python.file_uploaded_to_sandbox",
                            filename=filename,
                            sandbox_path=sandbox_path,
                            size_bytes=len(file_content),
                            reused_sandbox=reused)

        # Install pip packages if specified
        # On reused sandbox, packages may already be installed but pip handles this
        if requirements:
            logger.info("tools.execute_python.installing_packages",
                        requirements=requirements,
                        reused_sandbox=reused)
            install_output = sandbox.commands.run(f"pip install {requirements}")
            logger.info("tools.execute_python.packages_installed",
                        exit_code=install_output.exit_code,
                        stdout_length=len(install_output.stdout))

        # Execute code
        logger.info("tools.execute_python.executing_code",
                    code_length=len(code),
                    timeout=timeout)

        execution = sandbox.run_code(code=code, timeout=timeout)

        # Process execution results
        stdout_list = execution.logs.stdout if execution.logs else []
        stderr_list = execution.logs.stderr if execution.logs else []

        logger.info("tools.execute_python.execution_complete",
                    stdout_lines=len(stdout_list),
                    stderr_lines=len(stderr_list),
                    has_error=bool(execution.error),
                    has_results=bool(execution.results))

        stdout_str = "".join(stdout_list)
        stderr_str = "".join(stderr_list)
        error_str = str(execution.error) if execution.error else ""
        success = execution.error is None

        # Serialize results
        results_list = []
        if execution.results:
            for r in execution.results:
                logger.debug("tools.execute_python.result_object",
                             result_type=type(r).__name__,
                             result_str=str(r)[:200])
                results_list.append(str(r)[:1000])

        results_serialized = json.dumps(results_list, ensure_ascii=False)

        # Scan for generated files
        generated_files: List[Dict[str, Any]] = []

        try:
            logger.info("tools.execute_python.scanning_output_files")
            all_files = sandbox.files.list("/tmp")

            for entry in all_files:
                if not entry.name:
                    continue

                file_path = entry.path

                if file_path.startswith("/tmp/inputs/"):
                    continue

                if entry.name in ("inputs", ".ICE-unix", ".X11-unix"):
                    continue

                logger.info("tools.execute_python.downloading_output_file",
                            path=file_path,
                            name=entry.name)

                try:
                    file_bytes = sandbox.files.read(file_path, format="bytes")
                except Exception as read_error:  # pylint: disable=broad-exception-caught
                    logger.debug("tools.execute_python.skip_file",
                                 path=file_path,
                                 error=str(read_error))
                    continue

                # Detect MIME from magic bytes and extension (not just extension)
                mime_type = detect_mime_type(
                    filename=entry.name,
                    file_bytes=file_bytes,
                )

                generated_files.append({
                    "filename": entry.name,
                    "path": file_path,
                    "size": len(file_bytes),
                    "mime_type": mime_type,
                    "content": file_bytes
                })

                logger.info("tools.execute_python.output_file_found",
                            filename=entry.name,
                            size=len(file_bytes),
                            mime_type=mime_type)

            logger.info("tools.execute_python.output_files_scanned",
                        file_count=len(generated_files))

        except Exception as scan_error:  # pylint: disable=broad-exception-caught
            logger.info("tools.execute_python.output_scan_failed",
                        error=str(scan_error))

        sandbox_end_time = time.time()
        sandbox_duration = sandbox_end_time - sandbox_start_time

        # Prepare result dict
        # Phase 3.2+: Return raw file data for async caching
        # Files will be stored in Redis and metadata returned to Claude
        result: Dict[str, Any] = {
            "stdout": stdout_str,
            "stderr": stderr_str,
            "results": results_serialized,
            "error": error_str,
            "success": str(success).lower(),
        }

        # Include raw file data for async processing
        # (will be converted to cached files with metadata)
        if generated_files:
            result["_raw_files"] = generated_files

        # Return sandbox_id for caching (do NOT kill on success)
        return result, sandbox_duration, sandbox.sandbox_id

    except Exception as exec_error:
        # Mark execution as failed so we kill sandbox in finally
        execution_failed = True
        raise exec_error

    finally:
        # Only kill sandbox on execution failure to avoid stale state
        # On success, sandbox is kept alive for reuse (E2B auto-expires after idle)
        if execution_failed:
            try:
                sandbox.kill()
                logger.info("tools.execute_python.sandbox_killed_on_error")
            except Exception as cleanup_error:  # pylint: disable=broad-exception-caught
                logger.info("tools.execute_python.sandbox_cleanup_failed",
                            error=str(cleanup_error))
        else:
            logger.debug("tools.execute_python.sandbox_kept_alive",
                         sandbox_id=sandbox.sandbox_id)


# pylint: disable=too-many-locals,too-many-statements
async def execute_python(code: str,
                         bot: 'Bot',
                         session: 'AsyncSession',
                         thread_id: Optional[int] = None,
                         file_inputs: Optional[List[Dict[str, str]]] = None,
                         requirements: Optional[str] = None,
                         timeout: Optional[float] = 3600.0) -> Dict[str, Any]:
    """Execute Python code in E2B sandbox with file support.

    Runs Python code in secure E2B sandbox (Linux, Python 3.11+, headless).
    Supports pip packages, internet access, input/output file operations.
    Sandboxes are ephemeral and destroyed after execution.

    Environment:
    - Working directory: /home/user
    - Input files (if file_inputs provided): /tmp/inputs/{filename}
    - Output files: Save to /tmp/ or subdirectories
    - Bot auto-detects and downloads all new files from /tmp/

    Args:
        code: Python code to execute. Can include imports, multiple
            lines, functions, classes, etc.
        bot: Telegram Bot instance for downloading user files.
        session: Database session for querying file metadata.
        file_inputs: Optional list of input files to upload to sandbox.
            Each item: {"file_id": "file_abc...", "name": "document.pdf"}.
            Files will be available at /tmp/inputs/{name}.
        requirements: Optional space-separated list of pip packages
            (e.g., "numpy pandas matplotlib requests").
        timeout: Maximum execution time in seconds. Default: 3600 seconds
            (1 hour). Can be reduced for faster feedback on simple tasks.

    Returns:
        Dictionary with execution results:
        - 'stdout': Standard output from the code.
        - 'stderr': Standard error output (warnings, errors).
        - 'results': Serialized results (matplotlib plots, return values).
        - 'error': Error message if execution failed, otherwise empty.
        - 'success': 'true' if no errors, 'false' otherwise.
        - 'output_files': List of cached files with metadata:
            [{"temp_id": "exec_abc123_output.pdf", "filename": "output.pdf",
              "size_bytes": 102400, "mime_type": "application/pdf",
              "preview": "PDF document, ~5 pages, 100.0 KB"}]
        - 'cost_usd': Execution cost in USD.

    Raises:
        Exception: If sandbox creation, file operations, or execution fails.

    Examples:
        >>> # Simple execution
        >>> result = await execute_python(code="print('Hello!')")
        >>> print(result['stdout'])
        Hello!

        >>> # With input files
        >>> result = await execute_python(
        ...     code=("import pandas as pd; "
        ...           "df = pd.read_csv('/tmp/inputs/data.csv'); "
        ...           "print(df.head())"),
        ...     file_inputs=[{"file_id": "file_xyz", "name": "data.csv"}],
        ...     requirements="pandas"
        ... )

        >>> # With output files
        >>> result = await execute_python(
        ...     code="with open('/tmp/output.txt', 'w') as f: f.write('Result')"
        ... )
        >>> print(result['generated_files'])
        [{"filename": "output.txt", "size": 6, "mime_type": "text/plain"}]
    """
    try:
        logger.info("tools.execute_python.called",
                    code_length=len(code),
                    file_inputs_count=len(file_inputs or []),
                    requirements=requirements,
                    timeout=timeout)

        # Early validation: detect missing file_inputs when code expects files
        # This provides a clear error message instead of FileNotFoundError
        if '/tmp/inputs/' in code and not file_inputs:
            logger.info("tools.execute_python.missing_file_inputs",
                        code_preview=code[:200])
            return {
                "stdout": "",
                "stderr": "",
                "results": "[]",
                "error": (
                    "Code references '/tmp/inputs/' but no file_inputs provided. "
                    "You must pass file_inputs parameter with file_id and name "
                    "from 'Available files' section to upload files to sandbox. "
                    "Example: file_inputs=[{\"file_id\": \"file_abc...\", "
                    "\"name\": \"data.csv\"}]"),
                "success": "false",
                "generated_files": "[]",
                "cost_usd": "0.000000",
            }

        # Step 1: Check for cached sandbox (allows reuse between calls)
        # pylint: disable=import-outside-toplevel
        from cache.sandbox_cache import cache_sandbox
        from cache.sandbox_cache import get_cached_sandbox
        from cache.sandbox_cache import invalidate_sandbox
        cached_sandbox_id = None
        if thread_id:
            cached_sandbox_id = await get_cached_sandbox(thread_id)

        # Step 2: Download all input files IN PARALLEL before sandbox
        # This allows event loop to handle keepalive during downloads
        # Phase 3.2+: Using unified FileManager for all downloads
        downloaded_files: Dict[str, bytes] = {}
        if file_inputs:
            logger.info("tools.execute_python.downloading_input_files",
                        file_count=len(file_inputs))

            # Use FileManager for parallel downloads with caching
            from core.file_manager import FileManager
            file_manager = FileManager(bot, session)
            downloaded_files = await file_manager.download_many_by_claude_id(
                file_inputs,
                use_cache=True,
            )

        # Step 3: Run sandbox in thread pool to avoid blocking event loop
        # This allows keepalive updates during long sandbox operations
        # Pass cached_sandbox_id for reuse (faster iterations)
        try:
            result, sandbox_duration, sandbox_id = await asyncio.to_thread(
                _run_sandbox_sync,
                code,
                downloaded_files,
                requirements,
                timeout or 3600.0,
                cached_sandbox_id,
            )

            # Cache sandbox for reuse on success
            if thread_id and sandbox_id:
                await cache_sandbox(thread_id, sandbox_id)

        except Exception as sandbox_error:
            # Invalidate cached sandbox on error (stale state)
            if thread_id:
                await invalidate_sandbox(thread_id)
            raise sandbox_error

        # Step 4: Add cost to result
        cost_usd = calculate_e2b_cost(sandbox_duration)
        result["cost_usd"] = f"{cost_to_float(cost_usd):.6f}"

        # Step 5: Process generated files - store in Redis cache IN PARALLEL
        # Phase 3.2+: Model decides whether to deliver files to user
        # Phase 3.2+: Parallelized caching for better latency
        raw_files = result.pop("_raw_files", [])
        output_files: List[Dict[str, Any]] = []

        if raw_files:
            import uuid as uuid_mod  # pylint: disable=import-outside-toplevel
            execution_id = uuid_mod.uuid4().hex[:8]

            # Create cache tasks for parallel execution
            async def cache_one(
                    file_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
                """Cache single file and return metadata.

                For image files, also includes base64 data for Claude to see.
                """
                filename = file_data["filename"]
                content = file_data["content"]
                mime_type = file_data["mime_type"]

                # Context describes what this file is (for model understanding)
                file_context = f"Python output: {filename}"

                metadata = await store_exec_file(
                    filename=filename,
                    content=content,
                    mime_type=mime_type,
                    context=file_context,
                    execution_id=execution_id,
                    thread_id=thread_id,
                )

                if metadata:
                    logger.info(
                        "tools.execute_python.file_cached",
                        temp_id=metadata["temp_id"],
                        filename=filename,
                        size_bytes=len(content),
                    )
                    file_info: Dict[str, Any] = {
                        "temp_id": metadata["temp_id"],
                        "filename": metadata["filename"],
                        "size_bytes": metadata["size_bytes"],
                        "mime_type": metadata["mime_type"],
                        "preview": metadata["preview"],
                    }
                    # Add base64 image data for Claude to see image previews
                    # Limited to images under 1MB to avoid context bloat
                    if mime_type.startswith("image/") and len(
                            content) < 1024 * 1024:
                        file_info["_image_data"] = base64.b64encode(
                            content).decode('utf-8')
                    return file_info
                else:
                    logger.info(
                        "tools.execute_python.file_cache_failed",
                        filename=filename,
                        size_bytes=len(content),
                    )
                    return None

            # Cache all files in parallel
            cache_results = await asyncio.gather(
                *[cache_one(fd) for fd in raw_files],
                return_exceptions=True,
            )

            # Collect successful results
            for result_item in cache_results:
                if isinstance(result_item, BaseException):
                    logger.info(
                        "tools.execute_python.file_cache_exception",
                        error=str(result_item),
                    )
                elif result_item is not None:
                    output_files.append(result_item)  # type: ignore[arg-type]

        # Add output_files to result (metadata only, no binary)
        # Extract image previews for Claude to see
        image_previews = []
        for file_info in output_files:
            image_data = file_info.pop("_image_data", None)
            if image_data:
                image_previews.append({
                    "data": image_data,
                    "media_type": file_info["mime_type"],
                    "filename": file_info["filename"],
                })

        result["output_files"] = output_files

        # Add image previews for Claude to visually verify generated images
        if image_previews:
            result["_image_previews"] = image_previews
            logger.info(
                "tools.execute_python.image_previews_included",
                count=len(image_previews),
                filenames=[p["filename"] for p in image_previews],
            )

        logger.info("tools.execute_python.success",
                    success=result["success"],
                    stdout_length=len(result["stdout"]),
                    stderr_length=len(result["stderr"]),
                    output_files_count=len(output_files),
                    has_error=bool(result["error"]),
                    sandbox_duration_seconds=round(sandbox_duration, 2),
                    cost_usd=cost_to_float(cost_usd))

        return result

    except Exception as e:
        # E2B sandbox failures are external service issues, not our bugs
        logger.info("tools.execute_python.failed", error=str(e))
        # Re-raise to let caller handle
        raise


# Tool definition for Claude API (anthropic tools format)
# Phase 1.5 Stage 6: Added cache_control for tool caching optimization
# This is the LAST tool in TOOL_DEFINITIONS, so cache_control here
# enables caching for ALL tool definitions (~3,268 tokens)
EXECUTE_PYTHON_TOOL = {
    "name":
        "execute_python",
    "description":
        """Execute Python code in secure E2B sandbox with file I/O support.

Use this when you need to: run code, perform calculations, analyze data,
make HTTP requests, process files, generate files (PDF/PNG/CSV/etc.),
or use Python libraries. Full internet + pip install support.

<environment>
E2B sandbox - Debian Linux, Python 3.11+, headless:
- Working directory: /home/user
- Input files (if file_inputs provided): /tmp/inputs/{filename}
- Output files: Save to /tmp/ or subdirectories (e.g., /tmp/output.pdf)
- System: Full Debian with apt-get, subprocess for shell commands
- Pre-installed: Python standard library, pip, basic CLI tools
</environment>

<system_package_installation>
You can install ANY system packages via apt-get (sandbox is Debian-based).
This capability is crucial for tasks requiring specialized tools like document
conversion (libreoffice), media processing (ffmpeg), or image manipulation (imagemagick).

Installation command:
  subprocess.run('apt-get update && apt-get install -y package-name',
                 shell=True, capture_output=True)

Examples: libreoffice, ffmpeg, imagemagick, pandoc, texlive, ghostscript, etc.
Check availability: subprocess.run(['which', 'command'], ...)
List packages: subprocess.run(['dpkg', '-l'], ...)
Installation typically takes 10-60 seconds depending on package size.
</system_package_installation>

<file_input_output>
INPUT FILES:
Specify file_inputs with file_id and name from "Available files" section.
Files uploaded to /tmp/inputs/ before execution.
Example: file_inputs=[{"file_id": "file_abc...", "name": "document.pdf"}]
In code: open('/tmp/inputs/document.pdf', 'rb')

OUTPUT FILES:
Save to /tmp/ (any format: PDF, PNG, CSV, XLSX, TXT, etc.)
Files are cached temporarily (30 min TTL) and returned as output_files metadata.
YOU DECIDE whether to deliver files to user based on:
- User's explicit request ("create a chart", "generate PDF")
- File quality and correctness (verify before delivering)
- Relevance to conversation

To deliver: use deliver_file tool with temp_id from output_files.
Example: deliver_file(temp_id="exec_abc123_chart.png")

output_files format:
[{"temp_id": "exec_abc123_chart.png", "filename": "chart.png",
  "size_bytes": 45678, "mime_type": "image/png",
  "preview": "Image 800x600 (RGB), 44.6 KB"}]

Preview helps you understand file content without seeing binary data.
</file_input_output>

<verification_workflow>
VERIFY BEFORE DELIVERING - especially after negative feedback:

After generating files, verify the result BEFORE delivering to user.
Verification catches issues that code-level checks miss (corrupted files,
encoding problems, incomplete conversions, wrong output format).

**Verification process:**
1. Check output_files preview (size, dimensions, format info)
2. For images <1MB: base64 preview is included in tool results ‚Äî examine it!
3. If preview insufficient or need detailed analysis:
   - Use preview_file(file_id="exec_xxx", question="...") for ANY file type
   - preview_file handles images/PDFs by uploading to Files API internally
   - This does NOT send the file to user ‚Äî it's YOUR internal verification
4. If problems found, iterate with different approach
5. Only deliver_file when YOU are confident file is correct

**Key point:** preview_file does NOT deliver to user. Use it freely for verification.
deliver_file is what sends to user ‚Äî call it only after verification.

**Example workflow (PDF generation with verification):**
Turn 1: execute_python ‚Üí output_files: [{temp_id: "exec_abc_out.pdf", preview: "PDF, 8KB"}]
        Preview shows suspiciously small size
Turn 2: preview_file(file_id="exec_abc_out.pdf", question="Is this complete?")
        ‚Üí "PDF has only 2 pages with placeholder text"
Turn 3: execute_python (fix code) ‚Üí output_files: [{preview: "PDF, 245KB"}]
Turn 4: preview_file ‚Üí confirms content is correct
Turn 5: deliver_file(temp_id=...) ‚Üí sends PDF to user ‚úì
</verification_workflow>

<workflow_examples>
Example 1 - Data analysis with chart:
User: "Analyze data.csv and create a chart"
1. execute_python(file_inputs=[...], code="...plt.savefig('/tmp/chart.png')...")
2. Result: output_files: [{"temp_id": "exec_abc_chart.png", "preview": "Image 800x600..."}]
3. deliver_file(temp_id="exec_abc_chart.png") ‚Üí sends chart to user

Example 2 - Iterative approach:
User: "Convert presentation.pptx to PDF"
1. execute_python(conversion code) ‚Üí output_files shows small file
2. execute_python(with libreoffice) ‚Üí output_files shows proper size
3. deliver_file(temp_id=...) ‚Üí sends final PDF to user
</workflow_examples>

<key_features>
- Secure isolated environment with sandbox reuse (packages/files persist within thread)
- Internet access (API calls, web scraping, downloads)
- Pip packages (numpy, pandas, matplotlib, requests, pillow, etc.)
- System packages (libreoffice, ffmpeg, imagemagick, pandoc, etc.)
- File processing (read/write/convert any format)
- Return: stdout, stderr, results, output_files metadata
- Use preview_file to verify output, deliver_file to send to user
</key_features>

<when_to_use>
Use when user asks to: run code, perform calculations, analyze data, make HTTP requests,
process files, generate files (reports/charts/images), use Python libraries,
data transformations, file format conversions, image processing, or any Python-based task.
</when_to_use>

<when_not_to_use>
Do NOT use for: simple arithmetic (use built-in capabilities), tasks not requiring code
execution, or when user explicitly asks NOT to run code.
</when_not_to_use>

<limitations>
- 1 hour default timeout (3600 seconds, can be reduced for simple tasks)
- Sandbox reused within thread (packages/files persist up to 1 hour idle)
- Limited CPU/RAM (1 vCPU, reasonable memory)
- Headless (no GUI/display output, but can save to files)
</limitations>

COST: ~$0.05/hour of runtime. Typical execution: <1 second = <$0.0001.
Free tier: $100 credit (~2000 hours).""",
    "input_schema": {
        "type": "object",
        "properties": {
            "code": {
                "type":
                    "string",
                "description": (
                    "Python code to execute. Can include imports, multiple "
                    "lines, functions, classes, etc. Input files at /tmp/inputs/, "
                    "save outputs to /tmp/. Use print() for debug output.")
            },
            "file_inputs": {
                "type":
                    "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "file_id": {
                            "type":
                                "string",
                            "description": (
                                "claude_file_id from 'Available files' section "
                                "in system prompt (e.g., 'file_abc123...').")
                        },
                        "name": {
                            "type":
                                "string",
                            "description": (
                                "Original filename from 'Available files' "
                                "(e.g., 'document.pdf'). File will be available "
                                "at /tmp/inputs/{name} in sandbox.")
                        }
                    },
                    "required": ["file_id", "name"]
                },
                "description":
                    ("List of input files to upload to sandbox. "
                     "Use file_id and name from 'Available files' section. "
                     "Files uploaded to /tmp/inputs/ before code execution. "
                     "Optional - omit if no file inputs needed.")
            },
            "requirements": {
                "type":
                    "string",
                "description": (
                    "Space-separated list of pip packages to install before "
                    "execution (e.g., 'numpy pandas matplotlib requests pillow'). "
                    "Only packages not in Python standard library. Optional.")
            },
            "timeout": {
                "type":
                    "number",
                "description": (
                    "Maximum execution time in seconds. Default: 3600 (1 hour). "
                    "Can be reduced for faster feedback on simple tasks. Optional."
                )
            }
        },
        "required": ["code"]
    },
    # Phase 1.5 Stage 6: Enable prompt caching for all tool definitions
    # This MUST be on the LAST tool in TOOL_DEFINITIONS list
    # Caches all 5 tools (~3,268 tokens) with 1-hour ephemeral cache
    # Cost: 10x reduction on cache hits ($0.15 ‚Üí $0.015 per 1M tokens)
    # Shared across ALL users (same tool definitions for everyone)
    # 1-hour TTL: 2x write cost but lasts 12x longer ‚Üí ~41% savings vs 5-min
    "cache_control": {
        "type": "ephemeral",
        "ttl": "1h"  # 1-hour cache for better cost efficiency
    }
}


def format_execute_python_result(
    tool_input: Dict[str, Any],
    result: Dict[str, Any],
) -> str:
    """Format execute_python result for user display.

    Args:
        tool_input: The input parameters (code, file_inputs, etc.).
        result: The result dictionary with stdout, stderr, success, error.

    Returns:
        Formatted system message string.
    """
    if result.get("success") == "true":
        stdout = result.get("stdout", "").strip()
        if stdout:
            # Truncate long output
            preview = stdout[:100] + "..." if len(stdout) > 100 else stdout
            return f"[‚úÖ –ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω: {preview}]"
        return "[‚úÖ –ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ]"
    else:
        error = result.get("error", "unknown error")
        # Truncate long error
        preview = error[:80] + "..." if len(error) > 80 else error
        return f"[‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {preview}]"


# Unified tool configuration
# Import here to avoid circular dependencies at module level
from core.tools.base import ToolConfig  # pylint: disable=wrong-import-position

TOOL_CONFIG = ToolConfig(
    name="execute_python",
    definition=EXECUTE_PYTHON_TOOL,
    executor=execute_python,
    emoji="üêç",
    needs_bot_session=True,
    format_result=format_execute_python_result,
)
