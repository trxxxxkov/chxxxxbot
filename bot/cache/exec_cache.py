"""Execution output cache for temporary file storage.

This module provides caching for files generated by execute_python tool.
Files are stored temporarily so the model can analyze execution results
and decide whether to deliver files to the user.

Key features:
- 30-minute TTL (model should decide quickly)
- Up to 100MB per file
- Stores both file content and metadata
- Automatic file preview generation

NO __init__.py - use direct import:
    from cache.exec_cache import (
        store_exec_file, get_exec_file, get_exec_meta, delete_exec_file
    )
"""

import json
import time
from typing import Any, List, Optional
import uuid

from cache.client import get_redis
from cache.keys import exec_file_key
from cache.keys import EXEC_FILE_MAX_SIZE
from cache.keys import EXEC_FILE_TTL
from cache.keys import exec_meta_key
from cache.keys import exec_thread_index_key
from utils.metrics import record_cache_operation
from utils.metrics import record_redis_operation_time
from utils.structured_logging import get_logger

logger = get_logger(__name__)


def generate_temp_id(filename: str) -> str:
    """Generate unique temporary ID for execution output file.

    Uses only UUID to avoid Claude mistyping complex filenames with dates.
    The filename is stored in metadata and retrieved when delivering.

    Args:
        filename: Original filename (unused, kept for API compatibility).

    Returns:
        Short temporary ID (e.g., "exec_a1b2c3d4").
    """
    _ = filename  # Unused, filename stored in metadata
    short_uuid = uuid.uuid4().hex[:8]
    return f"exec_{short_uuid}"


def _generate_preview(  # pylint: disable=too-many-return-statements
    filename: str,
    content: bytes,
    mime_type: str,
) -> str:
    """Generate human-readable preview of file content.

    Args:
        filename: Original filename.
        content: File content as bytes.
        mime_type: MIME type of the file.

    Returns:
        Preview string describing file content.
    """
    size_kb = len(content) / 1024
    size_mb = size_kb / 1024

    # Size description
    if size_mb >= 1:
        size_str = f"{size_mb:.1f} MB"
    elif size_kb >= 1:
        size_str = f"{size_kb:.1f} KB"
    else:
        size_str = f"{len(content)} bytes"

    # Image files
    if mime_type.startswith("image/"):
        try:
            import io  # pylint: disable=import-outside-toplevel

            from PIL import Image  # pylint: disable=import-outside-toplevel
            img = Image.open(io.BytesIO(content))
            width, height = img.size
            mode = img.mode
            return f"Image {width}x{height} ({mode}), {size_str}"
        except Exception:  # pylint: disable=broad-exception-caught
            return f"Image file, {size_str}"

    # PDF files
    if mime_type == "application/pdf":
        try:
            # Try to count pages
            content_str = content[:10000].decode("latin-1", errors="ignore")
            page_count = content_str.count("/Type /Page") - content_str.count(
                "/Type /Pages")
            if page_count > 0:
                return f"PDF document, ~{page_count} pages, {size_str}"
        except Exception:  # pylint: disable=broad-exception-caught
            pass
        return f"PDF document, {size_str}"

    # Text files
    if mime_type.startswith("text/") or mime_type in (
            "application/json",
            "application/xml",
            "application/javascript",
    ):
        try:
            text = content.decode("utf-8")
            lines = text.count("\n") + 1
            chars = len(text)
            # First line preview
            first_line = text.split("\n")[0][:50]
            if len(text.split("\n")[0]) > 50:
                first_line += "..."
            return f"Text file, {lines} lines, {chars} chars: \"{first_line}\""
        except Exception:  # pylint: disable=broad-exception-caught
            return f"Text file, {size_str}"

    # CSV files
    if mime_type == "text/csv" or filename.endswith(".csv"):
        try:
            text = content.decode("utf-8")
            csv_lines = text.strip().split("\n")
            cols = len(csv_lines[0].split(",")) if csv_lines else 0
            rows = len(csv_lines)
            header = csv_lines[0][:60] if csv_lines else ""
            if csv_lines and len(csv_lines[0]) > 60:
                header += "..."
            return f"CSV file, {rows} rows x {cols} cols, header: {header}"
        except Exception:  # pylint: disable=broad-exception-caught
            return f"CSV file, {size_str}"

    # Excel files
    if mime_type in (
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-excel",
    ) or filename.endswith((".xlsx", ".xls")):
        return f"Excel spreadsheet, {size_str}"

    # Archive files
    if mime_type in ("application/zip", "application/x-tar",
                     "application/gzip"):
        return f"Archive file, {size_str}"

    # Audio/video
    if mime_type.startswith("audio/"):
        return f"Audio file, {size_str}"
    if mime_type.startswith("video/"):
        return f"Video file, {size_str}"

    # Default
    return f"Binary file ({mime_type}), {size_str}"


async def store_exec_file(
    filename: str,
    content: bytes,
    mime_type: str,
    context: str,
    execution_id: Optional[str] = None,
    thread_id: Optional[int] = None,
    delivery_hint: Optional[str] = None,
) -> Optional[dict]:
    """Store execution output file in Redis.

    Args:
        filename: Original filename (e.g., "plot.png").
        content: File content as bytes.
        mime_type: MIME type of the file.
        context: Human-readable description of what this file is
            (e.g., "LaTeX: E=mc^2", "Python plot: sales chart").
            This helps the model understand the file's purpose.
        execution_id: Optional execution ID for grouping.
        thread_id: Optional thread ID for associating file with conversation.
        delivery_hint: Optional hint for how to deliver the file.
            "document" for uncompressed delivery, None for default (photo).

    Returns:
        Metadata dict with temp_id if stored successfully, None otherwise.
    """
    # Check size limit (normal constraint)
    if len(content) > EXEC_FILE_MAX_SIZE:
        logger.info(
            "exec_cache.file_too_large",
            filename=filename,
            size_bytes=len(content),
            max_size=EXEC_FILE_MAX_SIZE,
        )
        return None

    start_time = time.time()
    redis = await get_redis()

    if redis is None:
        # Graceful degradation - file won't be cached but tool continues
        logger.info("exec_cache.redis_unavailable", filename=filename)
        return None

    try:
        # Generate temp ID
        temp_id = generate_temp_id(filename)

        # Generate preview
        preview = _generate_preview(filename, content, mime_type)

        # Prepare metadata
        metadata = {
            "temp_id": temp_id,
            "filename": filename,
            "size_bytes": len(content),
            "mime_type": mime_type,
            "preview": preview,
            "context": context,
            "execution_id": execution_id,
            "thread_id": thread_id,
            "delivery_hint": delivery_hint,
            "created_at": time.time(),
            "expires_at": time.time() + EXEC_FILE_TTL,
        }

        # Store file content (convert bytearray to bytes for Redis)
        file_key = exec_file_key(temp_id)
        file_content = bytes(content) if isinstance(content,
                                                    bytearray) else content
        await redis.setex(file_key, EXEC_FILE_TTL, file_content)

        # Store metadata
        meta_key = exec_meta_key(temp_id)
        await redis.setex(meta_key, EXEC_FILE_TTL, json.dumps(metadata))

        # Add to thread index for O(1) lookup (if thread_id provided)
        if thread_id is not None:
            thread_key = exec_thread_index_key(thread_id)
            await redis.sadd(thread_key, temp_id)
            # Set TTL on index (same as file TTL, refreshed on each add)
            await redis.expire(thread_key, EXEC_FILE_TTL)

        elapsed = time.time() - start_time
        record_redis_operation_time("set", elapsed)

        logger.info(
            "exec_cache.file_stored",
            temp_id=temp_id,
            filename=filename,
            size_bytes=len(content),
            mime_type=mime_type,
            preview=preview[:100],
            ttl=EXEC_FILE_TTL,
            elapsed_ms=elapsed * 1000,
        )

        return metadata

    except Exception as e:  # pylint: disable=broad-exception-caught
        logger.error(
            "exec_cache.store_error",
            filename=filename,
            error=str(e),
            exc_info=True,
        )
        return None


async def get_exec_file(temp_id: str) -> Optional[bytes]:
    """Get execution output file content from Redis.

    Args:
        temp_id: Temporary file ID.

    Returns:
        File content as bytes if found, None otherwise.
    """
    start_time = time.time()
    redis = await get_redis()

    if redis is None:
        return None

    try:
        key = exec_file_key(temp_id)
        content = await redis.get(key)

        elapsed = time.time() - start_time
        record_redis_operation_time("get", elapsed)

        if content is None:
            record_cache_operation("exec_file", hit=False)
            logger.debug("exec_cache.file_miss", temp_id=temp_id)
            return None

        record_cache_operation("exec_file", hit=True)
        logger.debug(
            "exec_cache.file_hit",
            temp_id=temp_id,
            size_bytes=len(content),
            elapsed_ms=elapsed * 1000,
        )

        return content

    except Exception as e:  # pylint: disable=broad-exception-caught
        logger.error(
            "exec_cache.get_error",
            temp_id=temp_id,
            error=str(e),
        )
        return None


async def get_exec_meta(temp_id: str) -> Optional[dict]:
    """Get execution output file metadata from Redis.

    Args:
        temp_id: Temporary file ID.

    Returns:
        Metadata dict if found, None otherwise.
    """
    redis = await get_redis()

    if redis is None:
        return None

    try:
        key = exec_meta_key(temp_id)
        data = await redis.get(key)

        if data is None:
            return None

        return json.loads(data.decode("utf-8"))

    except Exception as e:  # pylint: disable=broad-exception-caught
        logger.error(
            "exec_cache.get_meta_error",
            temp_id=temp_id,
            error=str(e),
        )
        return None


async def delete_exec_file(temp_id: str) -> bool:
    """Delete execution output file from Redis.

    Also removes from thread index if thread_id was set.

    Args:
        temp_id: Temporary file ID.

    Returns:
        True if deleted, False otherwise.
    """
    redis = await get_redis()

    if redis is None:
        return False

    try:
        # Get metadata first to know thread_id for index cleanup
        meta_key = exec_meta_key(temp_id)
        meta_data = await redis.get(meta_key)
        thread_id = None
        if meta_data:
            try:
                metadata = json.loads(meta_data.decode("utf-8"))
                thread_id = metadata.get("thread_id")
            except (json.JSONDecodeError, AttributeError):
                pass

        # Delete file and metadata
        file_key = exec_file_key(temp_id)
        deleted = await redis.delete(file_key, meta_key)

        # Remove from thread index if thread_id was set
        if thread_id is not None:
            thread_key = exec_thread_index_key(thread_id)
            await redis.srem(thread_key, temp_id)

        logger.debug(
            "exec_cache.file_deleted",
            temp_id=temp_id,
            keys_deleted=deleted,
            thread_id=thread_id,
        )

        return deleted > 0

    except Exception as e:  # pylint: disable=broad-exception-caught
        logger.error(
            "exec_cache.delete_error",
            temp_id=temp_id,
            error=str(e),
        )
        return False


async def get_pending_files_for_thread(thread_id: int) -> List[dict]:
    """Get all pending (not yet delivered) files for a thread.

    Uses thread index (SET) for O(1) lookup instead of SCAN.
    Files are ordered by creation time (oldest first).

    Args:
        thread_id: Thread ID to filter files by.

    Returns:
        List of metadata dicts for pending files. Each dict contains:
        - temp_id: Temporary ID for delivery
        - filename: Original filename
        - size_bytes: File size
        - mime_type: MIME type
        - preview: Human-readable preview
        - created_at: Unix timestamp
        - expires_at: Unix timestamp when file expires
    """
    redis = await get_redis()

    if redis is None:
        # Graceful degradation - returns empty list
        logger.info("exec_cache.get_pending_files.redis_unavailable",
                    thread_id=thread_id)
        return []

    try:
        # Get temp_ids from thread index (O(1) lookup)
        thread_key = exec_thread_index_key(thread_id)
        temp_ids = await redis.smembers(thread_key)

        if not temp_ids:
            logger.debug("exec_cache.get_pending_files.empty",
                         thread_id=thread_id)
            return []

        # Batch get metadata for all temp_ids
        pending_files: List[dict] = []
        stale_ids: List[str] = []  # IDs in index but metadata expired

        for temp_id_bytes in temp_ids:
            temp_id = (temp_id_bytes.decode("utf-8") if isinstance(
                temp_id_bytes, bytes) else temp_id_bytes)

            try:
                meta_key = exec_meta_key(temp_id)
                data = await redis.get(meta_key)

                if data is None:
                    # Metadata expired but still in index - mark for cleanup
                    stale_ids.append(temp_id)
                    continue

                metadata = json.loads(data.decode("utf-8"))
                pending_files.append(metadata)

            except Exception as e:  # pylint: disable=broad-exception-caught
                logger.debug("exec_cache.get_pending_files.parse_error",
                             temp_id=temp_id,
                             error=str(e))
                continue

        # Lazy cleanup: remove stale IDs from index
        if stale_ids:
            await redis.srem(thread_key, *stale_ids)
            logger.debug("exec_cache.get_pending_files.cleanup",
                         thread_id=thread_id,
                         stale_count=len(stale_ids))

        # Sort by created_at (oldest first)
        pending_files.sort(key=lambda f: f.get("created_at", 0))

        logger.info("exec_cache.get_pending_files.success",
                    thread_id=thread_id,
                    file_count=len(pending_files))

        return pending_files

    except Exception as e:  # pylint: disable=broad-exception-caught
        logger.error("exec_cache.get_pending_files.error",
                     thread_id=thread_id,
                     error=str(e),
                     exc_info=True)
        return []
